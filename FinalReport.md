# Final Report -Bikash Aryal, 04/28/2024

# Topic: RNASeq Data Analysis Pipelines for the Publicly available dataset

## 1. Introduction
Androgen-deprivation therapy (ADT) is standard treatment for locally advanced or metastatic prostate cancer (PCa). After approximately 2–3 years, patients can develop castration-resistant PCa (CRPC), for which the prognosis is poor despite newer second-line cytotoxic chemotherapy and endocrine therapies. The molecular mechanisms underlying CRPC progression are unclear. Therefore, the objective of original study was to undertake quantitative tumour transcriptome profiling prior to and following ADT to identify functionally important androgen-regulated pathways or genes that may be reactivated in CRPC. In the original study design; tumour-rich, targeted prostatic biopsies were taken from seven patients with locally advanced or metastatic PCa before and approximately 22 wk after ADT initiation. Illumina RNA-seq was performed, and complementary DNA sample library normalization was conducted using the Illumina duplex-specific nuclease protocol. The resulting data were sequenced using the HiSeq 2000 sequencer in a paired-end sequencing strategy. The published paper detailing this study can be accessed [here](https://www.sciencedirect.com/science/article/pii/S0302283813008324#sec0010). The [dataset](https://www.ncbi.nlm.nih.gov/bioproject/?term=prjna209978) comprises 14 biosamples from 7 patients, both pre and post ADT treatment.  I have used this dataset to develop two separate pipelines capable of analyzing similar publicly accessible RNASeq data.

## 2. Scope of the project

There are numerous RNASeq datasets available on NCBI/ENSEMBL. Nonetheless, importing these datasets into our working directory and preparing them for downstream analysis can pose certain challenges. This project is focused on creating pipelines for RNASeq data analysis by importing and analyzing publicly available datasets. Furthermore, I plan to employ these pipelines for the RNASeq data analysis in my own project.

## 3. Pipeline-I

This pipeline has three subdirectories namely “results”, “run” and “scripts”. There is runner script “run.sh” inside the “run” directory which run all the primary scripts (.sh) from “script” directory, typically submitting those as batch jobs, and often using loops to do so. All the outputs generated by running those scripts were out-directed and stored on “result” directory. Now, let go with each of the steps of runner script (run.sh) to discuss what this step does and what is generates.
 
- First step defines the SRR IDs from 14 bio-samples, loop over each of the IDs and submit a separate job for each of those IDs.  The script (data.sh) used in this step load the SRA toolkit and download the SRA file with “prefetch” command and convert them to FASTQ format with “fasterq_dump”. Finally, downloaded FASTQ files in _1.fastq and _2.fastq were first gun-zipped and reformatted to _R1.fastq.gz and _R2.fastq.gz respectively, with the command “basename”.  

- Second step download the reference sequence in FASTA and GTF format from Ensembl using the “wget” command and unzip them.

- Third step defines inputs (for FASTQ, FASTA and GTF files), settings and outputs (specifies the path where the output count table from featureCounts will be saved).

- Fourth step is for the quality control with the FastQC where the loop iterates through each FASTQ file. Here, fastqc.sh script ensures that FastQC is executed correctly with the specified input file and output directory within a Slurm job environment, handling error cases and providing informative messages about the process and its outcome. It runs FastQC on the input FASTQ file (fastqc --outdir "$outdir" "$fastq_file"), specifying the output directory for FastQC results.

- Fifth step launches trimgalore.sh script for each _R1 FASTQ file. performs paired-end trimming and quality control using TrimGalore, inferring the R2 FASTQ file name, creating output directories, running the tool, and renaming output files to standardized names. 

- Sixth step launches the star_index.sh script to generate a genome index using STAR for the specified assembly FASTA file and annotation GTF file. This script processes command-line arguments, sets up the required software environment using Conda, runs STAR to generate the genome index, and reports the completion status along with the STAR version used. This indexing step is crucial for subsequent alignment and analysis of sequencing data against the reference genome.

- Seventh step iterates over R1 FASTQ files and submits jobs to align paired-end reads using STAR (star_align.sh script). This script processes command-line arguments, sets up the necessary software environment using Conda, infers the R2 FASTQ file name, runs STAR to align reads against the indexed genome, and reports the completion status along with the STAR version used.

- Next step submits a job to execute the featurecounts.sh script, which counts the number of reads or read pairs that overlap with annotated genomic features (genes) using FeatureCounts. This script processes command-line arguments, sets up the necessary software environment using Conda, runs FeatureCounts to generate a gene count table (counts.tsv) from STAR-aligned BAM files, and reports the completion status along with the software version used. The resulting gene count table (counts.tsv) can be used for further analysis and visualization in bioinformatics workflows.

- Final step submits a job to execute the multiqc.sh script, which generates a comprehensive QC report (multiqc_report.html) summarizing the results of various analysis pipelines stored in the input directory (results) using MultiQC. This resulting MultiQC report can be viewed/downloaded and used for further analysis and interpretation.

### You can access this pipeline-I at OSC with this following path directory: 

/fs/ess/PAS2700/users/aryalb/project/pipeline

## Pipeline -II

In this scenario, I employed the same data from Pipeline -I and executed it using the nf-core Nextflow pipeline. However, running this pipeline took longer, and I encountered timeouts each time at OSC. OSC currently restricts jobs to a maximum of four hours. I also attempted to run this pipeline with fewer samples, but encountered issues. This pipeline represents a highly convenient approach for analyzing short reads against reference genomes to map reads to genes. The inputs of this pipeline are FASTQ files with raw reads, and reference genome files (assembly & annotation), while the outputs include a gene count table and many “QC outputs”. Similar with the pipeline I, I have used a primary script (scripts/nfc-rnaseq.sh) that will be submitted as a batch job, and a runner script (run/run.sh) with commands to run interactively. Again, let's proceed with each step outlined in the runner script (run.sh): 

- Activates a specific Conda environment using miniconda3/23.3.1-py310
- Sets an environment variable for the Singularity container cache directory
- Downloads the nf-core RNA-seq pipeline (rnaseq) files with specific configurations
- Defines pipeline outputs (outdir) and working directory (workdir” and inputs. We need to prepare a sample sheet as one of its inputs where we provide the paths to your FASTQ files and the so-called “strandedness” of our RNA-Seq library
- Creates a configuration file for batch job submissions (nextflow.config)
- Finally submits the nfc-rnaseq.sh script as a batch job (sbatch), passing required arguments. 

    The nfc-rnaseq.sh script configures and executes an RNA-seq workflow using the nf-core framework within a SLURM-managed environment. It sets SLURM job parameters for job submission and monitoring, loads the required Conda environment for Nextflow, and defines the Singularity cache directory. This script validates input arguments, creates necessary output directories (outdir and workdir), and initiates the Nextflow workflow (nextflow run) with specified parameters including the sample sheet, reference FASTA and GTF files, output directory, and workflow settings. It also includes options to remove ribosomal RNA sequences, specifies the use of Singularity for containerization, disables ANSI color codes in log messages for compatibility (-ansi-log false), and resumes execution from the last checkpoint if available (-resume).

### You can access this pipeline-II at OSC with this following path directory: 
/fs/ess/PAS2700/users/aryalb/project/nfc-rnaseq 

In summary, the described RNA-seq analysis pipelines exemplify comprehensive workflows for processing raw sequencing data into interpretable results, with a focus on reproducibility, scalability, and efficient utilization of computational resources. To implement these pipelines for your specific case, you'll need to load various bioinformatic packages from OSC and conda, acquire raw sequencing data (FASTQ files), and reference sequences (FASTA, GTF files). Both of these pipelines have some similarity in utilizing the common bioinformatics tools (FastQC, STAR, FeatureCounts, MultiQC) to perform key analysis steps. Pipeline-I relies on a custom shell script approach for job submission and orchestration, while Pipeline-II leverages the nf-core Nextflow framework for workflow management and execution.

